{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn.dqn import QNetwork, ReplayMemory\n",
    "from policies.eps_greedy import EpsilonGreedyPolicy\n",
    "from train_eval.train import train\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kylia\\Anaconda3\\envs\\rl2020\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DQN and policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42  # This is not randomly chosen\n",
    "num_hidden = 128\n",
    "eps = 0.05\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "Q_net = QNetwork(num_hidden=num_hidden)\n",
    "policy = EpsilonGreedyPolicy(Q_net, eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to run episodes and call training function (with replay memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(train, Q, policy, memory, env, num_episodes, batch_size, discount_factor, optimizer):\n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        steps = 0\n",
    "        while True:\n",
    "            # Set epsilon according to number of steps\n",
    "            policy.set_epsilon(global_steps)\n",
    "            # Sample an action, next state, reward and done\n",
    "            a = policy.sample_action(state)\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            # Add the transition to the memory buffer\n",
    "            memory.push((state, a, r, s_next, done))\n",
    "            # Perform training on the buffer\n",
    "            loss = train(Q, memory, optimizer, batch_size, discount_factor)\n",
    "            # Increase step counts and set current state\n",
    "            steps += 1\n",
    "            global_steps += 1\n",
    "            state = s_next\n",
    "\n",
    "            if done:\n",
    "                if i % 10 == 0:\n",
    "                    print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                          .format(i, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "                episode_durations.append(steps)\n",
    "                break\n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 0 finished after 8 steps\n",
      " Episode 10 finished after 22 steps\n",
      " Episode 20 finished after 11 steps\n",
      " Episode 30 finished after 15 steps\n",
      " Episode 40 finished after 17 steps\n",
      " Episode 50 finished after 66 steps\n",
      " Episode 60 finished after 169 steps\n",
      " Episode 70 finished after 129 steps\n",
      " Episode 80 finished after 122 steps\n",
      " Episode 90 finished after 130 steps\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100\n",
    "batch_size = 64\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "# To switch off Replay Memory mechanism, simply set size to batch size\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "optimizer = optim.Adam(Q_net.parameters(), learn_rate)\n",
    "episode_durations = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to run episodes and call training function (without replay memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(train, Q, policy, memory, env, num_episodes, batch_size, discount_factor, optimizer):\n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        steps = 0\n",
    "        while True:\n",
    "            # Set epsilon according to number of steps\n",
    "            policy.set_epsilon(global_steps)\n",
    "            # Increase step counts\n",
    "            steps += 1\n",
    "            global_steps += 1\n",
    "            # Sample an action, next state, reward and done\n",
    "            a = policy.sample_action(state)\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            # Add the transition to the memory buffer\n",
    "            memory.push((state, a, r, s_next, done))\n",
    "            # Perform training on the buffer\n",
    "            if global_steps % batch_size == 0:\n",
    "                loss = train(Q, memory, optimizer, batch_size, discount_factor)\n",
    "            state = s_next\n",
    "\n",
    "            if done:\n",
    "                if i % 500 == 0:\n",
    "                    print(\"{2} Episode {0} finished after {1} steps\"\n",
    "                          .format(i, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "                episode_durations.append(steps)\n",
    "                break\n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Episode 0 finished after 8 steps\n",
      " Episode 500 finished after 10 steps\n",
      " Episode 1000 finished after 9 steps\n",
      " Episode 1500 finished after 12 steps\n",
      " Episode 2000 finished after 9 steps\n",
      " Episode 2500 finished after 8 steps\n",
      " Episode 3000 finished after 9 steps\n",
      " Episode 3500 finished after 9 steps\n",
      " Episode 4000 finished after 220 steps\n",
      " Episode 4500 finished after 10 steps\n",
      " Episode 5000 finished after 174 steps\n",
      " Episode 5500 finished after 158 steps\n",
      " Episode 6000 finished after 161 steps\n",
      " Episode 6500 finished after 194 steps\n",
      " Episode 7000 finished after 170 steps\n",
      " Episode 7500 finished after 11 steps\n",
      " Episode 8000 finished after 31 steps\n",
      " Episode 8500 finished after 102 steps\n",
      " Episode 9000 finished after 13 steps\n",
      " Episode 9500 finished after 125 steps\n"
     ]
    }
   ],
   "source": [
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "Q_net = QNetwork(num_hidden=num_hidden)\n",
    "policy = EpsilonGreedyPolicy(Q_net, eps)\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 32\n",
    "discount_factor = 0.8\n",
    "learn_rate = 1e-3\n",
    "memory = ReplayMemory(batch_size)\n",
    "\n",
    "optimizer = optim.Adam(Q_net.parameters(), learn_rate)\n",
    "episode_durations = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
