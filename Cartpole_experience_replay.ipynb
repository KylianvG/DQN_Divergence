{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn.dqn import QNetwork, ReplayMemory\n",
    "from policies.eps_greedy import EpsilonGreedyPolicy\n",
    "from train_eval.train import train\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to run episodes and call training function (memory off, baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(train, Q, policy, memory, env, num_episodes, batch_size, discount_factor, optimizer):\n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    state_values = []\n",
    "    init_state = env.reset()\n",
    "    for i in tqdm(range(1, num_episodes+1)):\n",
    "        state = env.reset()\n",
    "\n",
    "        steps = 0\n",
    "        while True:\n",
    "            # Set epsilon according to number of steps\n",
    "            policy.set_epsilon(global_steps)\n",
    "            # Increase step counts\n",
    "            steps += 1\n",
    "            global_steps += 1\n",
    "            # Sample an action, next state, reward and done\n",
    "            a = policy.sample_action(state)\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            # Add the transition to the memory buffer\n",
    "            memory.push((state, a, r, s_next, done))\n",
    "            # Perform training on the buffer\n",
    "            if global_steps % batch_size == 0:\n",
    "                loss = train(Q, memory, optimizer, batch_size, discount_factor)\n",
    "            state = s_next\n",
    "            \n",
    "            if done:\n",
    "                episode_durations.append(steps)\n",
    "                break\n",
    "        if i == 1 or i % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                Q.eval()\n",
    "                Q_vals = Q(torch.Tensor([init_state]))\n",
    "                maxx, _ = torch.max(Q_vals, dim=1)\n",
    "                state_values.append(maxx)\n",
    "    return episode_durations, state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network (Replay memory off, baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e5f70798e540b68d9a53d74d2470d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 42  # This is not randomly chosen\n",
    "num_hidden = 128\n",
    "eps = 0.05\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "Q_net = QNetwork(num_hidden=num_hidden)\n",
    "policy = EpsilonGreedyPolicy(Q_net, eps)\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 64\n",
    "discount_factor = 0.9\n",
    "learn_rate = 1e-3\n",
    "\n",
    "# We switch off Replay Memory mechanism by setting the memory size to batch size\n",
    "# This is our baseline \n",
    "memory = ReplayMemory(batch_size)\n",
    "\n",
    "optimizer = optim.Adam(Q_net.parameters(), learn_rate)\n",
    "episode_durations, state_values = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "plt.plot(smooth(np.array(state_values), 10))\n",
    "plt.ylabel('$\\max_a\\ Q(s,a)$')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(smooth(np.array(episode_durations), 10))\n",
    "plt.ylabel('Episode duration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to run episodes and call training function (memory on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes_memory(train, Q, policy, memory, env, num_episodes, batch_size, discount_factor, optimizer):\n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    state_values = []\n",
    "    init_state = env.reset()\n",
    "    print(memory.capacity)\n",
    "    for i in tqdm(range(1, num_episodes+1)):\n",
    "        state = env.reset()\n",
    "\n",
    "        steps = 0\n",
    "        while True:\n",
    "            # Set epsilon according to number of steps\n",
    "            policy.set_epsilon(global_steps)\n",
    "            # Increase step counts\n",
    "            steps += 1\n",
    "            global_steps += 1\n",
    "            # Sample an action, next state, reward and done\n",
    "            a = policy.sample_action(state)\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            # Add the transition to the memory buffer\n",
    "            memory.push((state, a, r, s_next, done))\n",
    "            # Perform training on the buffer\n",
    "            loss = train(Q, memory, optimizer, batch_size, discount_factor)\n",
    "            state = s_next\n",
    "            \n",
    "            if done:\n",
    "                episode_durations.append(steps)\n",
    "                break\n",
    "        if i == 1 or i % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                Q.eval()\n",
    "                Q_vals = Q(torch.Tensor([init_state]))\n",
    "                maxx, _ = torch.max(Q_vals, dim=1)\n",
    "                state_values.append(maxx)\n",
    "    return episode_durations, state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network (now replay memory on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42  # This is not randomly chosen\n",
    "num_hidden = 128\n",
    "eps = 0.05\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "Q_net = QNetwork(num_hidden=num_hidden)\n",
    "policy = EpsilonGreedyPolicy(Q_net, eps)\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 64\n",
    "discount_factor = 0.9\n",
    "learn_rate = 1e-3\n",
    "\n",
    "# We first test Replay Memory by setting the memory size to 10.000 \n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "optimizer = optim.Adam(Q_net.parameters(), learn_rate)\n",
    "episode_durations, state_values = run_episodes_memory(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(smooth(np.array(state_values), 10))\n",
    "plt.ylabel('$\\max_a\\ Q(s,a)$')\n",
    "plt.show()\n",
    "print(len(state_values), len(episode_durations))\n",
    "plt.plot(smooth(np.array(episode_durations), 10))\n",
    "plt.ylabel('Episode duration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of concept done, time for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed hyperparams among experiments\n",
    "num_hidden = 128\n",
    "eps = 0.05\n",
    "num_episodes = 10000\n",
    "batch_size = 64\n",
    "discount_factor = 0.9\n",
    "learn_rate = 1e-3\n",
    "\n",
    "# Number of experiments to average results over\n",
    "num_experiments = 1\n",
    "\n",
    "# Memory sizes to be tested\n",
    "test_memory_sizes = [64, 100, 500, 1000, 10000]\n",
    "\n",
    "# Make buffers for the cumulative results\n",
    "cum_state_values = {i : [] for i in test_memory_sizes}\n",
    "cum_episode_durations = {i : [] for i in test_memory_sizes}\n",
    "\n",
    "for memory_size in test_memory_sizes:\n",
    "    print(\"Current memory size:\", memory_size)\n",
    "    for i in tqdm(range(num_experiments)):\n",
    "        \n",
    "        # Set the seed such that each update frequency is tested on the same seeds\n",
    "        random.seed(i)\n",
    "        torch.manual_seed(i)\n",
    "        env.seed(i)\n",
    "        \n",
    "        # Create network, policy and replaymemory (which is not used; renewed every batch)\n",
    "        Q_net = QNetwork(num_hidden=num_hidden)\n",
    "        policy = EpsilonGreedyPolicy(Q_net, eps)\n",
    "        \n",
    "        # Use current test memory size\n",
    "        memory = ReplayMemory(memory_size)\n",
    "\n",
    "        optimizer = optim.Adam(Q_net.parameters(), learn_rate)\n",
    "        \n",
    "        if memory_size == batch_size:\n",
    "            # Replay memory off\n",
    "            print(\"Replay memory off\")\n",
    "            episode_durations, state_values = run_episodes(\n",
    "                    train,\n",
    "                    Q_net, policy,\n",
    "                    memory,\n",
    "                    env,\n",
    "                    num_episodes,\n",
    "                    batch_size,\n",
    "                    discount_factor,\n",
    "                    optimizer,\n",
    "            )\n",
    "        else:\n",
    "            episode_durations, state_values = run_episodes_memory(\n",
    "                    train,\n",
    "                    Q_net, policy,\n",
    "                    memory,\n",
    "                    env,\n",
    "                    num_episodes,\n",
    "                    batch_size,\n",
    "                    discount_factor,\n",
    "                    optimizer,\n",
    "            )\n",
    "        \n",
    "        cum_state_values[memory_size].append(np.array(state_values))\n",
    "        cum_episode_durations[memory_size].append(np.array(episode_durations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average over the cumulative results\n",
    "for size in test_memory_sizes:\n",
    "    cum_state_values[size] = np.array(cum_state_values[size])\n",
    "    cum_episode_durations[size]  = np.array(cum_episode_durations[size])\n",
    "    \n",
    "print(cum_state_values.keys())\n",
    "print(cum_state_values[64])\n",
    "print(cum_state_values[100])\n",
    "print(cum_state_values[1000])\n",
    "\n",
    "# Plot initial state value over time\n",
    "for size in test_memory_sizes:\n",
    "    stds = np.std(cum_state_values[size], axis=0)\n",
    "    mean_values = np.mean(cum_state_values[size], axis=0)\n",
    "    plt.fill_between(np.linspace(1, (num_episodes//100)+1, num=(num_episodes//100)+1), mean_values-stds, mean_values+stds, alpha=0.3)\n",
    "    plt.plot(mean_values)\n",
    "plt.ylabel('$\\max_a\\ Q(s_0,a)$')\n",
    "plt.xlabel('Episode')\n",
    "figure_memory_sizes = [\"Off\", 100, 500, 1000, 10000]\n",
    "plt.legend([f'Memory = {size}' for size in figure_memory_sizes], loc=2)\n",
    "ticks = [0, 20, 40, 60, 80, 100]\n",
    "plt.xticks(ticks, [t*100 for t in ticks])\n",
    "plt.show()\n",
    "\n",
    "# Plot episode duration over time\n",
    "for size in test_memory_sizes:\n",
    "    smooth_stds = smooth(np.std(cum_episode_durations[size], axis=0), 100)\n",
    "    smooth_mean_values = smooth(np.mean(cum_episode_durations[size], axis=0), 100)\n",
    "    plt.fill_between(np.linspace(1, len(smooth_stds), num=len(smooth_stds)), smooth_mean_values-smooth_stds, smooth_mean_values+smooth_stds, alpha=0.3)\n",
    "    plt.plot(smooth_mean_values)\n",
    "plt.ylabel('Episode duration')\n",
    "plt.xlabel('Episode')\n",
    "plt.legend([f'Memory = {size}' for size in figure_memory_sizes], loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
