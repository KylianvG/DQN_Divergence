{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn.dqn import QNetwork, ReplayMemory\n",
    "from policies.eps_greedy import EpsilonGreedyPolicy\n",
    "from train_eval.train import train\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to run episodes and call training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episodes(train, Q, policy, memory, env, num_episodes, batch_size, discount_factor, optimizer):\n",
    "    global_steps = 0  # Count the steps (do not reset at episode start, to compute epsilon)\n",
    "    episode_durations = []  #\n",
    "    state_values = []\n",
    "    init_state = env.reset()\n",
    "    for i in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "\n",
    "        steps = 0\n",
    "        while True:\n",
    "            # Set epsilon according to number of steps\n",
    "            policy.set_epsilon(global_steps)\n",
    "            # Increase step counts\n",
    "            steps += 1\n",
    "            global_steps += 1\n",
    "            # Sample an action, next state, reward and done\n",
    "            a = policy.sample_action(state)\n",
    "            s_next, r, done, _ = env.step(a)\n",
    "            # Add the transition to the memory buffer\n",
    "            memory.push((state, a, r, s_next, done))\n",
    "            # Perform training on the buffer\n",
    "            if global_steps % batch_size == 0:\n",
    "                loss = train(Q, memory, optimizer, batch_size, discount_factor)\n",
    "            state = s_next\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                with torch.no_grad():\n",
    "                    Q.eval()\n",
    "                    Q_vals = Q(torch.Tensor([init_state]))\n",
    "                    maxx, _ = torch.max(Q_vals, dim=1)\n",
    "                    state_values.append(maxx)\n",
    "\n",
    "            if done:\n",
    "#                 if i % 100 == 0:\n",
    "#                     print(\"{2} Episode {0} finished after {1} steps\"\n",
    "#                           .format(i, steps, '\\033[92m' if steps >= 195 else '\\033[99m'))\n",
    "                episode_durations.append(steps)\n",
    "                break\n",
    "    return episode_durations, state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network (Baseline = with replay memory off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seed = 42  # This is not randomly chosen\n",
    "num_hidden = 128\n",
    "eps = 0.05\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "Q_net = QNetwork(num_hidden=num_hidden)\n",
    "policy = EpsilonGreedyPolicy(Q_net, eps)\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 64\n",
    "discount_factor = 0.9\n",
    "learn_rate = 1e-3\n",
    "\n",
    "# We switch off Replay Memory mechanism by setting the memory size to batch size\n",
    "# This is our baseline \n",
    "memory = ReplayMemory(batch_size)\n",
    "\n",
    "optimizer = optim.Adam(Q_net.parameters(), learn_rate)\n",
    "episode_durations, state_values = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "plt.plot(smooth(np.array(state_values), 10))\n",
    "plt.ylabel('$\\max_a\\ Q(s,a)$')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(smooth(np.array(episode_durations), 10))\n",
    "plt.ylabel('Episode duration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network (now replay memory on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42  # This is not randomly chosen\n",
    "num_hidden = 128\n",
    "eps = 0.05\n",
    "\n",
    "# We will seed the algorithm (before initializing QNetwork!) for reproducibility\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "env.seed(seed)\n",
    "\n",
    "Q_net = QNetwork(num_hidden=num_hidden)\n",
    "policy = EpsilonGreedyPolicy(Q_net, eps)\n",
    "\n",
    "num_episodes = 10000\n",
    "batch_size = 64\n",
    "discount_factor = 0.9\n",
    "learn_rate = 1e-3\n",
    "\n",
    "# We first test Replay Memory by setting the memory size to 10.000 \n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "optimizer = optim.Adam(Q_net.parameters(), learn_rate)\n",
    "episode_durations, state_values = run_episodes(train, Q_net, policy, memory, env, num_episodes, batch_size, discount_factor, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(smooth(np.array(state_values), 10))\n",
    "plt.ylabel('$\\max_a\\ Q(s,a)$')\n",
    "plt.show()\n",
    "print(len(state_values), len(episode_durations))\n",
    "plt.plot(smooth(np.array(episode_durations), 10))\n",
    "plt.ylabel('Episode duration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of concept done, time for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed hyperparams among experiments\n",
    "num_hidden = 128\n",
    "eps = 0.05\n",
    "num_episodes = 10000\n",
    "batch_size = 64\n",
    "discount_factor = 0.9\n",
    "learn_rate = 1e-3\n",
    "\n",
    "# Number of experiments to average results over\n",
    "num_experiments = 10\n",
    "\n",
    "# Memory sizes to be tested\n",
    "test_memory_sizes = [100, 500, 1000, 10000]\n",
    "\n",
    "# Make buffers for the cumulative results\n",
    "cum_state_values = {i : [] for i in test_memory_sizes}\n",
    "cum_episode_durations = {i : [] for i in test_memory_sizes}\n",
    "\n",
    "for memory_size in test_memory_sizes:\n",
    "    for i in tqdm(range(num_experiments)):\n",
    "        \n",
    "        # Set the seed such that each update frequency is tested on the same seeds\n",
    "        random.seed(i)\n",
    "        torch.manual_seed(i)\n",
    "        env.seed(i)\n",
    "        \n",
    "        # Create network, policy and replaymemory (which is not used; renewed every batch)\n",
    "        Q_net = QNetwork(num_hidden=num_hidden)\n",
    "        policy = EpsilonGreedyPolicy(Q_net, eps)\n",
    "        \n",
    "        # Use current test memory size\n",
    "        memory = ReplayMemory(memory_size)\n",
    "\n",
    "        optimizer = optim.Adam(Q_net.parameters(), learn_rate)\n",
    "        episode_durations, state_values = run_episodes(\n",
    "                train,\n",
    "                Q_net, policy,\n",
    "                memory,\n",
    "                env,\n",
    "                num_episodes,\n",
    "                batch_size,\n",
    "                discount_factor,\n",
    "                optimizer,\n",
    "        )\n",
    "        \n",
    "        cum_state_values[memory_size].append(np.array(state_values))\n",
    "        cum_episode_durations[memory_size].append(np.array(episode_durations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average over the cumulative results\n",
    "for size in test_memory_sizes:\n",
    "    cum_state_values[size] = np.array(cum_state_values[size])\n",
    "    cum_episode_durations[size]  = np.array(cum_episode_durations[size])\n",
    "\n",
    "for size in test_memory_sizes:\n",
    "    plt.plot(smooth(np.mean(cum_state_values[size], axis=0), 100))\n",
    "plt.ylabel('$\\max_a\\ Q(s_0,a)$')\n",
    "plt.xlabel('Episode')\n",
    "plt.legend([f'Memory = {size}' for size in test_memory_sizes])\n",
    "plt.show()\n",
    "\n",
    "for size in test_memory_sizes:\n",
    "    plt.plot(smooth(np.mean(cum_episode_durations[size], axis=0), 100))\n",
    "plt.ylabel('Episode duration')\n",
    "plt.xlabel('Episode')\n",
    "plt.legend([f'Memory = {size}' for size in test_memory_sizes])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
